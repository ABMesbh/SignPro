{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "class HandModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(63, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = nn.functional.softmax(x, dim=0)\n",
    "        return x\n",
    "\n",
    "model = HandModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploads\\a.txt\n",
      "uploads\\b.txt\n",
      "uploads\\c.txt\n",
      "uploads\\d.txt\n",
      "uploads\\data.csv\n",
      "uploads\\echec.txt\n",
      "file uploads\\a.txt\n",
      "file uploads\\a.txt done\n",
      "file uploads\\b.txt\n",
      "file uploads\\b.txt done\n",
      "file uploads\\c.txt\n",
      "file uploads\\c.txt done\n",
      "file uploads\\d.txt\n",
      "file uploads\\d.txt done\n",
      "file uploads\\echec.txt\n",
      "file uploads\\echec.txt done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "def getalldatafile():\n",
    "    res=[]\n",
    "    for file in os.listdir(\"uploads\"):\n",
    "        file=os.path.join(\"uploads\",file)\n",
    "        print(file)\n",
    "        if file.endswith(\".txt\") and file!=\"uploads\\classes.txt\":\n",
    "            res.append(str(file.split(\".\")[0]))\n",
    "    return res\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=['label'] + [f'd_{i:03}' for i in range(63)])\n",
    "df.to_csv(\"uploads\\data.csv\", index=False,mode='w')\n",
    "\n",
    "filenb=0\n",
    "classes={}\n",
    "for name in getalldatafile():\n",
    "    print(\"file \"+name+\".txt\")\n",
    "    classes[name.removeprefix(\"uploads\\\\\")]=filenb\n",
    "    filenb+=1\n",
    "    for line in open(name+\".txt\", \"r\").readlines():\n",
    "        with open(\"uploads\\data.csv\", \"a\") as f:\n",
    "            f.write(str(classes[name.removeprefix(\"uploads\\\\\")])+\",\"+line.removesuffix(\"\\n\")+\"\\n\")\n",
    "    print(\"file \"+name+\".txt done\")\n",
    "\n",
    "# ajout fichier de classe\n",
    "with open(\"uploads\\classes.txt\", \"w\") as f:\n",
    "    for key in classes.keys():\n",
    "        f.write(key.removeprefix(\"uploads\\\\\")+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the CSV file\n",
    "data = pd.read_csv(\"uploads\\data.csv\")\n",
    "\n",
    "# Separate the labels and the inputs\n",
    "labels = data['label'].values\n",
    "inputs = data.drop(columns=['label']).values\n",
    "\n",
    "# Convert the inputs and labels to PyTorch tensors\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(inputs_tensor, labels_tensor)\n",
    "train_ds,test_ds = torch.utils.data.random_split(dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))])\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Accuracy: 0.17\n",
      "Accuracy of class a: 0.21\n",
      "Accuracy of class b: 0.46\n",
      "Accuracy of class c: 0.03\n",
      "Accuracy of class d: 0.00\n",
      "Accuracy of class echec: 0.13\n"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy():\n",
    "    iclasses = {v: k for k, v in classes.items()}\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = {class_name: 0 for class_name in classes}\n",
    "    class_total = {class_name: 0 for class_name in classes}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dl:\n",
    "            outputs = model(inputs)\n",
    "            _,predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            for label, prediction in zip(labels, predicted):\n",
    "                if label == prediction:\n",
    "                    class_correct[iclasses[label.item()]] += 1\n",
    "                class_total[iclasses[label.item()]] += 1\n",
    "    \n",
    "    global_accuracy = correct / total\n",
    "    class_accuracy = {class_name: class_correct[class_name] / class_total[class_name] for class_name in classes}\n",
    "    \n",
    "    print(f'Global Accuracy: {global_accuracy:.2f}')\n",
    "    for class_name, accuracy in class_accuracy.items():\n",
    "        print(f'Accuracy of class {class_name}: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "evaluate_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TRAINING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Accuracy: 0.18\n",
      "Accuracy of class a: 0.20\n",
      "Accuracy of class b: 0.47\n",
      "Accuracy of class c: 0.01\n",
      "Accuracy of class d: 0.00\n",
      "Accuracy of class echec: 0.16\n",
      "Epoch 1/400, Loss: 1.4628323316574097\n",
      "Epoch 2/400, Loss: 1.4879530668258667\n",
      "Epoch 3/400, Loss: 1.4596450328826904\n",
      "Epoch 4/400, Loss: 1.4563589096069336\n",
      "Epoch 5/400, Loss: 1.461188793182373\n",
      "Epoch 6/400, Loss: 1.4595087766647339\n",
      "Epoch 7/400, Loss: 1.4555799961090088\n",
      "Epoch 8/400, Loss: 1.4581468105316162\n",
      "Epoch 9/400, Loss: 1.4550267457962036\n",
      "Epoch 10/400, Loss: 1.4560660123825073\n",
      "Epoch 11/400, Loss: 1.4559375047683716\n",
      "Epoch 12/400, Loss: 1.4543983936309814\n",
      "Epoch 13/400, Loss: 1.455106496810913\n",
      "Epoch 14/400, Loss: 1.454464077949524\n",
      "Epoch 15/400, Loss: 1.4548965692520142\n",
      "Epoch 16/400, Loss: 1.4579129219055176\n",
      "Epoch 17/400, Loss: 1.4556982517242432\n",
      "Epoch 18/400, Loss: 1.453982949256897\n",
      "Epoch 19/400, Loss: 1.4542109966278076\n",
      "Epoch 20/400, Loss: 1.4546654224395752\n",
      "Epoch 21/400, Loss: 1.4546535015106201\n",
      "Epoch 22/400, Loss: 1.4545294046401978\n",
      "Epoch 23/400, Loss: 1.4553648233413696\n",
      "Epoch 24/400, Loss: 1.4536089897155762\n",
      "Epoch 25/400, Loss: 1.456153154373169\n",
      "Epoch 26/400, Loss: 1.4539086818695068\n",
      "Epoch 27/400, Loss: 1.4536681175231934\n",
      "Epoch 28/400, Loss: 1.4552487134933472\n",
      "Epoch 29/400, Loss: 1.4540605545043945\n",
      "Epoch 30/400, Loss: 1.4539706707000732\n",
      "Epoch 31/400, Loss: 1.456170678138733\n",
      "Epoch 32/400, Loss: 1.455729365348816\n",
      "Epoch 33/400, Loss: 1.453568935394287\n",
      "Epoch 34/400, Loss: 1.4544780254364014\n",
      "Epoch 35/400, Loss: 1.4534281492233276\n",
      "Epoch 36/400, Loss: 1.4538183212280273\n",
      "Epoch 37/400, Loss: 1.4540787935256958\n",
      "Epoch 38/400, Loss: 1.4533066749572754\n",
      "Epoch 39/400, Loss: 1.453683614730835\n",
      "Epoch 40/400, Loss: 1.4589570760726929\n",
      "Epoch 41/400, Loss: 1.4542946815490723\n",
      "Epoch 42/400, Loss: 1.4533401727676392\n",
      "Epoch 43/400, Loss: 1.4559986591339111\n",
      "Epoch 44/400, Loss: 1.4536199569702148\n",
      "Epoch 45/400, Loss: 1.453366994857788\n",
      "Epoch 46/400, Loss: 1.4538793563842773\n",
      "Epoch 47/400, Loss: 1.453719973564148\n",
      "Epoch 48/400, Loss: 1.4533518552780151\n",
      "Epoch 49/400, Loss: 1.4533320665359497\n",
      "Epoch 50/400, Loss: 1.4535584449768066\n",
      "Epoch 51/400, Loss: 1.4533717632293701\n",
      "Epoch 52/400, Loss: 1.4532439708709717\n",
      "Epoch 53/400, Loss: 1.4547021389007568\n",
      "Epoch 54/400, Loss: 1.4533066749572754\n",
      "Epoch 55/400, Loss: 1.454167127609253\n",
      "Epoch 56/400, Loss: 1.4539787769317627\n",
      "Epoch 57/400, Loss: 1.4541267156600952\n",
      "Epoch 58/400, Loss: 1.4922550916671753\n",
      "Epoch 59/400, Loss: 1.4538825750350952\n",
      "Epoch 60/400, Loss: 1.4540067911148071\n",
      "Epoch 61/400, Loss: 1.4536774158477783\n",
      "Epoch 62/400, Loss: 1.4531350135803223\n",
      "Epoch 63/400, Loss: 1.4529920816421509\n",
      "Epoch 64/400, Loss: 1.4538780450820923\n",
      "Epoch 65/400, Loss: 1.4534733295440674\n",
      "Epoch 66/400, Loss: 1.4545166492462158\n",
      "Epoch 67/400, Loss: 1.4536019563674927\n",
      "Epoch 68/400, Loss: 1.4558840990066528\n",
      "Epoch 69/400, Loss: 1.4542672634124756\n",
      "Epoch 70/400, Loss: 1.4928089380264282\n",
      "Epoch 71/400, Loss: 1.4540742635726929\n",
      "Epoch 72/400, Loss: 1.4535053968429565\n",
      "Epoch 73/400, Loss: 1.4556450843811035\n",
      "Epoch 74/400, Loss: 1.4533134698867798\n",
      "Epoch 75/400, Loss: 1.4543191194534302\n",
      "Epoch 76/400, Loss: 1.4539847373962402\n",
      "Epoch 77/400, Loss: 1.4533131122589111\n",
      "Epoch 78/400, Loss: 1.4531663656234741\n",
      "Epoch 79/400, Loss: 1.4541478157043457\n",
      "Epoch 80/400, Loss: 1.4529907703399658\n",
      "Epoch 81/400, Loss: 1.4537025690078735\n",
      "Epoch 82/400, Loss: 1.4537646770477295\n",
      "Epoch 83/400, Loss: 1.453242301940918\n",
      "Epoch 84/400, Loss: 1.4560251235961914\n",
      "Epoch 85/400, Loss: 1.4926882982254028\n",
      "Epoch 86/400, Loss: 1.4532257318496704\n",
      "Epoch 87/400, Loss: 1.4533493518829346\n",
      "Epoch 88/400, Loss: 1.4565632343292236\n",
      "Epoch 89/400, Loss: 1.4565967321395874\n",
      "Epoch 90/400, Loss: 1.4555656909942627\n",
      "Epoch 91/400, Loss: 1.4535936117172241\n",
      "Epoch 92/400, Loss: 1.4531112909317017\n",
      "Epoch 93/400, Loss: 1.453640103340149\n",
      "Epoch 94/400, Loss: 1.4538644552230835\n",
      "Epoch 95/400, Loss: 1.4570016860961914\n",
      "Epoch 96/400, Loss: 1.4557071924209595\n",
      "Epoch 97/400, Loss: 1.4537638425827026\n",
      "Epoch 98/400, Loss: 1.4538946151733398\n",
      "Epoch 99/400, Loss: 1.4920915365219116\n",
      "Epoch 100/400, Loss: 1.4537800550460815\n",
      "Epoch 101/400, Loss: 1.4533413648605347\n",
      "Epoch 102/400, Loss: 1.45292329788208\n",
      "Epoch 103/400, Loss: 1.4534361362457275\n",
      "Epoch 104/400, Loss: 1.4536657333374023\n",
      "Epoch 105/400, Loss: 1.455663800239563\n",
      "Epoch 106/400, Loss: 1.4531219005584717\n",
      "Epoch 107/400, Loss: 1.4536454677581787\n",
      "Epoch 108/400, Loss: 1.453249454498291\n",
      "Epoch 109/400, Loss: 1.4533424377441406\n",
      "Epoch 110/400, Loss: 1.4546823501586914\n",
      "Epoch 111/400, Loss: 1.4541150331497192\n",
      "Epoch 112/400, Loss: 1.4538921117782593\n",
      "Epoch 113/400, Loss: 1.4531936645507812\n",
      "Epoch 114/400, Loss: 1.4539583921432495\n",
      "Epoch 115/400, Loss: 1.4528944492340088\n",
      "Epoch 116/400, Loss: 1.4539974927902222\n",
      "Epoch 117/400, Loss: 1.4923101663589478\n",
      "Epoch 118/400, Loss: 1.453177809715271\n",
      "Epoch 119/400, Loss: 1.4528288841247559\n",
      "Epoch 120/400, Loss: 1.453209638595581\n",
      "Epoch 121/400, Loss: 1.4547885656356812\n",
      "Epoch 122/400, Loss: 1.4536948204040527\n",
      "Epoch 123/400, Loss: 1.453218936920166\n",
      "Epoch 124/400, Loss: 1.4535256624221802\n",
      "Epoch 125/400, Loss: 1.4530068635940552\n",
      "Epoch 126/400, Loss: 1.4528875350952148\n",
      "Epoch 127/400, Loss: 1.4555025100708008\n",
      "Epoch 128/400, Loss: 1.4561787843704224\n",
      "Epoch 129/400, Loss: 1.4540460109710693\n",
      "Epoch 130/400, Loss: 1.453436017036438\n",
      "Epoch 131/400, Loss: 1.4531145095825195\n",
      "Epoch 132/400, Loss: 1.4532886743545532\n",
      "Epoch 133/400, Loss: 1.4539545774459839\n",
      "Epoch 134/400, Loss: 1.4540613889694214\n",
      "Epoch 135/400, Loss: 1.4563987255096436\n",
      "Epoch 136/400, Loss: 1.4560904502868652\n",
      "Epoch 137/400, Loss: 1.4528945684432983\n",
      "Epoch 138/400, Loss: 1.4535458087921143\n",
      "Epoch 139/400, Loss: 1.4532264471054077\n",
      "Epoch 140/400, Loss: 1.453168511390686\n",
      "Epoch 141/400, Loss: 1.4556770324707031\n",
      "Epoch 142/400, Loss: 1.454147458076477\n",
      "Epoch 143/400, Loss: 1.4557238817214966\n",
      "Epoch 144/400, Loss: 1.4535539150238037\n",
      "Epoch 145/400, Loss: 1.4532897472381592\n",
      "Epoch 146/400, Loss: 1.454920768737793\n",
      "Epoch 147/400, Loss: 1.4538531303405762\n",
      "Epoch 148/400, Loss: 1.4559825658798218\n",
      "Epoch 149/400, Loss: 1.4536312818527222\n",
      "Epoch 150/400, Loss: 1.4537646770477295\n",
      "Epoch 151/400, Loss: 1.4563931226730347\n",
      "Epoch 152/400, Loss: 1.4549320936203003\n",
      "Epoch 153/400, Loss: 1.453287959098816\n",
      "Epoch 154/400, Loss: 1.4927990436553955\n",
      "Epoch 155/400, Loss: 1.4536116123199463\n",
      "Epoch 156/400, Loss: 1.4534045457839966\n",
      "Epoch 157/400, Loss: 1.4532638788223267\n",
      "Epoch 158/400, Loss: 1.4530047178268433\n",
      "Epoch 159/400, Loss: 1.4531978368759155\n",
      "Epoch 160/400, Loss: 1.4545098543167114\n",
      "Epoch 161/400, Loss: 1.453102469444275\n",
      "Epoch 162/400, Loss: 1.4536672830581665\n",
      "Epoch 163/400, Loss: 1.4565616846084595\n",
      "Epoch 164/400, Loss: 1.4532901048660278\n",
      "Epoch 165/400, Loss: 1.4533414840698242\n",
      "Epoch 166/400, Loss: 1.4539257287979126\n",
      "Epoch 167/400, Loss: 1.4568169116973877\n",
      "Epoch 168/400, Loss: 1.4536888599395752\n",
      "Epoch 169/400, Loss: 1.4538966417312622\n",
      "Epoch 170/400, Loss: 1.4539210796356201\n",
      "Epoch 171/400, Loss: 1.455875039100647\n",
      "Epoch 172/400, Loss: 1.4541985988616943\n",
      "Epoch 173/400, Loss: 1.4534722566604614\n",
      "Epoch 174/400, Loss: 1.4545308351516724\n",
      "Epoch 175/400, Loss: 1.4539483785629272\n",
      "Epoch 176/400, Loss: 1.4533418416976929\n",
      "Epoch 177/400, Loss: 1.4558147192001343\n",
      "Epoch 178/400, Loss: 1.4533413648605347\n",
      "Epoch 179/400, Loss: 1.4921791553497314\n",
      "Epoch 180/400, Loss: 1.4538092613220215\n",
      "Epoch 181/400, Loss: 1.4534366130828857\n",
      "Epoch 182/400, Loss: 1.4537208080291748\n",
      "Epoch 183/400, Loss: 1.4530373811721802\n",
      "Epoch 184/400, Loss: 1.455004096031189\n",
      "Epoch 185/400, Loss: 1.4539179801940918\n",
      "Epoch 186/400, Loss: 1.4531991481781006\n",
      "Epoch 187/400, Loss: 1.4541449546813965\n",
      "Epoch 188/400, Loss: 1.4529234170913696\n",
      "Epoch 189/400, Loss: 1.4539016485214233\n",
      "Epoch 190/400, Loss: 1.4534766674041748\n",
      "Epoch 191/400, Loss: 1.453691840171814\n",
      "Epoch 192/400, Loss: 1.4559736251831055\n",
      "Epoch 193/400, Loss: 1.494950771331787\n",
      "Epoch 194/400, Loss: 1.4561272859573364\n",
      "Epoch 195/400, Loss: 1.4529801607131958\n",
      "Epoch 196/400, Loss: 1.4536654949188232\n",
      "Epoch 197/400, Loss: 1.4545068740844727\n",
      "Epoch 198/400, Loss: 1.453757882118225\n",
      "Epoch 199/400, Loss: 1.4567651748657227\n",
      "Epoch 200/400, Loss: 1.45282781124115\n",
      "Epoch 201/400, Loss: 1.4530019760131836\n",
      "Epoch 202/400, Loss: 1.4532995223999023\n",
      "Epoch 203/400, Loss: 1.4534744024276733\n",
      "Epoch 204/400, Loss: 1.454046368598938\n",
      "Epoch 205/400, Loss: 1.4542202949523926\n",
      "Epoch 206/400, Loss: 1.4530125856399536\n",
      "Epoch 207/400, Loss: 1.4533504247665405\n",
      "Epoch 208/400, Loss: 1.4536926746368408\n",
      "Epoch 209/400, Loss: 1.453882098197937\n",
      "Epoch 210/400, Loss: 1.455884575843811\n",
      "Epoch 211/400, Loss: 1.4534915685653687\n",
      "Epoch 212/400, Loss: 1.4535026550292969\n",
      "Epoch 213/400, Loss: 1.4557043313980103\n",
      "Epoch 214/400, Loss: 1.4536259174346924\n",
      "Epoch 215/400, Loss: 1.4536340236663818\n",
      "Epoch 216/400, Loss: 1.4531047344207764\n",
      "Epoch 217/400, Loss: 1.453474998474121\n",
      "Epoch 218/400, Loss: 1.4536093473434448\n",
      "Epoch 219/400, Loss: 1.4540551900863647\n",
      "Epoch 220/400, Loss: 1.4545063972473145\n",
      "Epoch 221/400, Loss: 1.4539809226989746\n",
      "Epoch 222/400, Loss: 1.4554650783538818\n",
      "Epoch 223/400, Loss: 1.4560579061508179\n",
      "Epoch 224/400, Loss: 1.4563833475112915\n",
      "Epoch 225/400, Loss: 1.4545131921768188\n",
      "Epoch 226/400, Loss: 1.4538196325302124\n",
      "Epoch 227/400, Loss: 1.4539213180541992\n",
      "Epoch 228/400, Loss: 1.4537254571914673\n",
      "Epoch 229/400, Loss: 1.4532194137573242\n",
      "Epoch 230/400, Loss: 1.4534863233566284\n",
      "Epoch 231/400, Loss: 1.4540902376174927\n",
      "Epoch 232/400, Loss: 1.4536305665969849\n",
      "Epoch 233/400, Loss: 1.4534014463424683\n",
      "Epoch 234/400, Loss: 1.4530695676803589\n",
      "Epoch 235/400, Loss: 1.4532089233398438\n",
      "Epoch 236/400, Loss: 1.4534088373184204\n",
      "Epoch 237/400, Loss: 1.4556862115859985\n",
      "Epoch 238/400, Loss: 1.4535845518112183\n",
      "Epoch 239/400, Loss: 1.453011155128479\n",
      "Epoch 240/400, Loss: 1.4564192295074463\n",
      "Epoch 241/400, Loss: 1.4536998271942139\n",
      "Epoch 242/400, Loss: 1.453343152999878\n",
      "Epoch 243/400, Loss: 1.4533333778381348\n",
      "Epoch 244/400, Loss: 1.453995943069458\n",
      "Epoch 245/400, Loss: 1.4559662342071533\n",
      "Epoch 246/400, Loss: 1.4530341625213623\n",
      "Epoch 247/400, Loss: 1.4920973777770996\n",
      "Epoch 248/400, Loss: 1.4534363746643066\n",
      "Epoch 249/400, Loss: 1.4555258750915527\n",
      "Epoch 250/400, Loss: 1.4555892944335938\n",
      "Epoch 251/400, Loss: 1.4531053304672241\n",
      "Epoch 252/400, Loss: 1.4530346393585205\n",
      "Epoch 253/400, Loss: 1.453908085823059\n",
      "Epoch 254/400, Loss: 1.4533458948135376\n",
      "Epoch 255/400, Loss: 1.4557702541351318\n",
      "Epoch 256/400, Loss: 1.4529261589050293\n",
      "Epoch 257/400, Loss: 1.4559390544891357\n",
      "Epoch 258/400, Loss: 1.4938340187072754\n",
      "Epoch 259/400, Loss: 1.4545038938522339\n",
      "Epoch 260/400, Loss: 1.4562376737594604\n",
      "Epoch 261/400, Loss: 1.4531582593917847\n",
      "Epoch 262/400, Loss: 1.454053521156311\n",
      "Epoch 263/400, Loss: 1.4566758871078491\n",
      "Epoch 264/400, Loss: 1.4529415369033813\n",
      "Epoch 265/400, Loss: 1.453871726989746\n",
      "Epoch 266/400, Loss: 1.454168438911438\n",
      "Epoch 267/400, Loss: 1.4540448188781738\n",
      "Epoch 268/400, Loss: 1.453342080116272\n",
      "Epoch 269/400, Loss: 1.4540597200393677\n",
      "Epoch 270/400, Loss: 1.4532240629196167\n",
      "Epoch 271/400, Loss: 1.4540908336639404\n",
      "Epoch 272/400, Loss: 1.4540095329284668\n",
      "Epoch 273/400, Loss: 1.4531853199005127\n",
      "Epoch 274/400, Loss: 1.4536316394805908\n",
      "Epoch 275/400, Loss: 1.4530106782913208\n",
      "Epoch 276/400, Loss: 1.4530491828918457\n",
      "Epoch 277/400, Loss: 1.4531583786010742\n",
      "Epoch 278/400, Loss: 1.4539895057678223\n",
      "Epoch 279/400, Loss: 1.4530344009399414\n",
      "Epoch 280/400, Loss: 1.4534339904785156\n",
      "Epoch 281/400, Loss: 1.4538474082946777\n",
      "Epoch 282/400, Loss: 1.4538060426712036\n",
      "Epoch 283/400, Loss: 1.455781102180481\n",
      "Epoch 284/400, Loss: 1.4532438516616821\n",
      "Epoch 285/400, Loss: 1.454139232635498\n",
      "Epoch 286/400, Loss: 1.4538846015930176\n",
      "Epoch 287/400, Loss: 1.4532334804534912\n",
      "Epoch 288/400, Loss: 1.453610897064209\n",
      "Epoch 289/400, Loss: 1.4537248611450195\n",
      "Epoch 290/400, Loss: 1.4556204080581665\n",
      "Epoch 291/400, Loss: 1.454623818397522\n",
      "Epoch 292/400, Loss: 1.4533450603485107\n",
      "Epoch 293/400, Loss: 1.4533694982528687\n",
      "Epoch 294/400, Loss: 1.4540246725082397\n",
      "Epoch 295/400, Loss: 1.453229546546936\n",
      "Epoch 296/400, Loss: 1.4544498920440674\n",
      "Epoch 297/400, Loss: 1.453942060470581\n",
      "Epoch 298/400, Loss: 1.4529201984405518\n",
      "Epoch 299/400, Loss: 1.4536110162734985\n",
      "Epoch 300/400, Loss: 1.4536654949188232\n",
      "Epoch 301/400, Loss: 1.4537206888198853\n",
      "Epoch 302/400, Loss: 1.4529212713241577\n",
      "Epoch 303/400, Loss: 1.4534955024719238\n",
      "Epoch 304/400, Loss: 1.4534090757369995\n",
      "Epoch 305/400, Loss: 1.454054355621338\n",
      "Epoch 306/400, Loss: 1.4564248323440552\n",
      "Epoch 307/400, Loss: 1.4919401407241821\n",
      "Epoch 308/400, Loss: 1.4587149620056152\n",
      "Epoch 309/400, Loss: 1.4556090831756592\n",
      "Epoch 310/400, Loss: 1.4528274536132812\n",
      "Epoch 311/400, Loss: 1.4537256956100464\n",
      "Epoch 312/400, Loss: 1.4533406496047974\n",
      "Epoch 313/400, Loss: 1.4536563158035278\n",
      "Epoch 314/400, Loss: 1.4536104202270508\n",
      "Epoch 315/400, Loss: 1.4540812969207764\n",
      "Epoch 316/400, Loss: 1.4543918371200562\n",
      "Epoch 317/400, Loss: 1.4529324769973755\n",
      "Epoch 318/400, Loss: 1.4532828330993652\n",
      "Epoch 319/400, Loss: 1.4535540342330933\n",
      "Epoch 320/400, Loss: 1.4540935754776\n",
      "Epoch 321/400, Loss: 1.453299880027771\n",
      "Epoch 322/400, Loss: 1.45607590675354\n",
      "Epoch 323/400, Loss: 1.4564083814620972\n",
      "Epoch 324/400, Loss: 1.4527673721313477\n",
      "Epoch 325/400, Loss: 1.4563796520233154\n",
      "Epoch 326/400, Loss: 1.4536454677581787\n",
      "Epoch 327/400, Loss: 1.4536683559417725\n",
      "Epoch 328/400, Loss: 1.4537256956100464\n",
      "Epoch 329/400, Loss: 1.4557678699493408\n",
      "Epoch 330/400, Loss: 1.4545038938522339\n",
      "Epoch 331/400, Loss: 1.456007957458496\n",
      "Epoch 332/400, Loss: 1.4560210704803467\n",
      "Epoch 333/400, Loss: 1.4552325010299683\n",
      "Epoch 334/400, Loss: 1.4537606239318848\n",
      "Epoch 335/400, Loss: 1.4921939373016357\n",
      "Epoch 336/400, Loss: 1.4531042575836182\n",
      "Epoch 337/400, Loss: 1.4537330865859985\n",
      "Epoch 338/400, Loss: 1.4533084630966187\n",
      "Epoch 339/400, Loss: 1.4542880058288574\n",
      "Epoch 340/400, Loss: 1.4531253576278687\n",
      "Epoch 341/400, Loss: 1.4532183408737183\n",
      "Epoch 342/400, Loss: 1.45420503616333\n",
      "Epoch 343/400, Loss: 1.455837607383728\n",
      "Epoch 344/400, Loss: 1.4537272453308105\n",
      "Epoch 345/400, Loss: 1.4529205560684204\n",
      "Epoch 346/400, Loss: 1.4563499689102173\n",
      "Epoch 347/400, Loss: 1.4538421630859375\n",
      "Epoch 348/400, Loss: 1.4533299207687378\n",
      "Epoch 349/400, Loss: 1.4537348747253418\n",
      "Epoch 350/400, Loss: 1.4531641006469727\n",
      "Epoch 351/400, Loss: 1.4539809226989746\n",
      "Epoch 352/400, Loss: 1.4532177448272705\n",
      "Epoch 353/400, Loss: 1.4540536403656006\n",
      "Epoch 354/400, Loss: 1.453794002532959\n",
      "Epoch 355/400, Loss: 1.4535515308380127\n",
      "Epoch 356/400, Loss: 1.454032301902771\n",
      "Epoch 357/400, Loss: 1.4533424377441406\n",
      "Epoch 358/400, Loss: 1.4549188613891602\n",
      "Epoch 359/400, Loss: 1.4534008502960205\n",
      "Epoch 360/400, Loss: 1.453304648399353\n",
      "Epoch 361/400, Loss: 1.455723762512207\n",
      "Epoch 362/400, Loss: 1.4555860757827759\n",
      "Epoch 363/400, Loss: 1.4535690546035767\n",
      "Epoch 364/400, Loss: 1.455988883972168\n",
      "Epoch 365/400, Loss: 1.4539501667022705\n",
      "Epoch 366/400, Loss: 1.4538699388504028\n",
      "Epoch 367/400, Loss: 1.454759120941162\n",
      "Epoch 368/400, Loss: 1.4563636779785156\n",
      "Epoch 369/400, Loss: 1.4539134502410889\n",
      "Epoch 370/400, Loss: 1.453964352607727\n",
      "Epoch 371/400, Loss: 1.4561952352523804\n",
      "Epoch 372/400, Loss: 1.4540096521377563\n",
      "Epoch 373/400, Loss: 1.4536668062210083\n",
      "Epoch 374/400, Loss: 1.4531073570251465\n",
      "Epoch 375/400, Loss: 1.4531844854354858\n",
      "Epoch 376/400, Loss: 1.4535504579544067\n",
      "Epoch 377/400, Loss: 1.496023416519165\n",
      "Epoch 378/400, Loss: 1.4541373252868652\n",
      "Epoch 379/400, Loss: 1.4536445140838623\n",
      "Epoch 380/400, Loss: 1.454095721244812\n",
      "Epoch 381/400, Loss: 1.4545854330062866\n",
      "Epoch 382/400, Loss: 1.4537441730499268\n",
      "Epoch 383/400, Loss: 1.453966498374939\n",
      "Epoch 384/400, Loss: 1.4543629884719849\n",
      "Epoch 385/400, Loss: 1.4546446800231934\n",
      "Epoch 386/400, Loss: 1.4544572830200195\n",
      "Epoch 387/400, Loss: 1.4533920288085938\n",
      "Epoch 388/400, Loss: 1.4535752534866333\n",
      "Epoch 389/400, Loss: 1.4537674188613892\n",
      "Epoch 390/400, Loss: 1.4558247327804565\n",
      "Epoch 391/400, Loss: 1.4555282592773438\n",
      "Epoch 392/400, Loss: 1.4537805318832397\n",
      "Epoch 393/400, Loss: 1.4528862237930298\n",
      "Epoch 394/400, Loss: 1.4529446363449097\n",
      "Epoch 395/400, Loss: 1.454044222831726\n",
      "Epoch 396/400, Loss: 1.4531145095825195\n",
      "Epoch 397/400, Loss: 1.4535174369812012\n",
      "Epoch 398/400, Loss: 1.4534012079238892\n",
      "Epoch 399/400, Loss: 1.456384539604187\n",
      "Epoch 400/400, Loss: 1.4532182216644287\n",
      "Global Accuracy: 0.97\n",
      "Accuracy of class a: 1.00\n",
      "Accuracy of class b: 1.00\n",
      "Accuracy of class c: 1.00\n",
      "Accuracy of class d: 0.99\n",
      "Accuracy of class echec: 0.92\n"
     ]
    }
   ],
   "source": [
    "evaluate_accuracy()\n",
    "# Training loop\n",
    "num_epochs = 400\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "evaluate_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "torch.save(model.state_dict(), 'uploads\\handmodel.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
