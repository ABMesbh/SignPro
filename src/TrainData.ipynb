{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "class HandModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(63, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = nn.functional.softmax(x, dim=0)\n",
    "        return x\n",
    "\n",
    "model = HandModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file a.txt done and removed\n",
      "file b.txt done and removed\n",
      "file c.txt done and removed\n",
      "file d.txt done and removed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "def getalldatafile():\n",
    "    res=[]\n",
    "    for file in os.listdir():\n",
    "        if file.endswith(\".txt\") and file!=\"classes.txt\":\n",
    "            res.append(str(file.split(\".\")[0]))\n",
    "    return res\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=['label'] + [f'd_{i:03}' for i in range(63)])\n",
    "df.to_csv(\"data.csv\", index=False,mode='w')\n",
    "\n",
    "filenb=0\n",
    "classes={}\n",
    "for name in getalldatafile():\n",
    "    classes[name]=filenb\n",
    "    filenb+=1\n",
    "    for line in open(name+\".txt\", \"r\").readlines():\n",
    "        with open(\"data.csv\", \"a\") as f:\n",
    "            f.write(str(classes[name])+\",\"+line.removesuffix(\"\\n\")+\"\\n\")\n",
    "    os.remove(name+\".txt\")\n",
    "    print(\"file \"+name+\".txt done and removed\")\n",
    "\n",
    "# ajout fichier de classe\n",
    "with open(\"classes.txt\", \"w\") as f:\n",
    "    for key in classes.keys():\n",
    "        f.write(key+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the CSV file\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Separate the labels and the inputs\n",
    "labels = data['label'].values\n",
    "inputs = data.drop(columns=['label']).values\n",
    "\n",
    "# Convert the inputs and labels to PyTorch tensors\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(inputs_tensor, labels_tensor)\n",
    "train_ds,test_ds = torch.utils.data.random_split(dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))])\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Accuracy: 0.16\n",
      "Accuracy of class a: 0.01\n",
      "Accuracy of class b: 0.09\n",
      "Accuracy of class c: 0.39\n",
      "Accuracy of class d: 0.13\n"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy():\n",
    "    iclasses = {v: k for k, v in classes.items()}\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = {class_name: 0 for class_name in classes}\n",
    "    class_total = {class_name: 0 for class_name in classes}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dl:\n",
    "            outputs = model(inputs)\n",
    "            _,predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            for label, prediction in zip(labels, predicted):\n",
    "                if label == prediction:\n",
    "                    class_correct[iclasses[label.item()]] += 1\n",
    "                class_total[iclasses[label.item()]] += 1\n",
    "    \n",
    "    global_accuracy = correct / total\n",
    "    class_accuracy = {class_name: class_correct[class_name] / class_total[class_name] for class_name in classes}\n",
    "    \n",
    "    print(f'Global Accuracy: {global_accuracy:.2f}')\n",
    "    for class_name, accuracy in class_accuracy.items():\n",
    "        print(f'Accuracy of class {class_name}: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "evaluate_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TRAINING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Accuracy: 0.18\n",
      "Accuracy of class a: 0.00\n",
      "Accuracy of class b: 0.08\n",
      "Accuracy of class c: 0.39\n",
      "Accuracy of class d: 0.20\n",
      "Epoch 1/400, Loss: 1.2289178371429443\n",
      "Epoch 2/400, Loss: 1.2300201654434204\n",
      "Epoch 3/400, Loss: 1.228576898574829\n",
      "Epoch 4/400, Loss: 1.2291069030761719\n",
      "Epoch 5/400, Loss: 1.2272191047668457\n",
      "Epoch 6/400, Loss: 1.2261853218078613\n",
      "Epoch 7/400, Loss: 1.2280702590942383\n",
      "Epoch 8/400, Loss: 1.233670949935913\n",
      "Epoch 9/400, Loss: 1.252577781677246\n",
      "Epoch 10/400, Loss: 1.2290358543395996\n",
      "Epoch 11/400, Loss: 1.2251989841461182\n",
      "Epoch 12/400, Loss: 1.226038932800293\n",
      "Epoch 13/400, Loss: 1.2275432348251343\n",
      "Epoch 14/400, Loss: 1.2256124019622803\n",
      "Epoch 15/400, Loss: 1.2251579761505127\n",
      "Epoch 16/400, Loss: 1.2252764701843262\n",
      "Epoch 17/400, Loss: 1.2268853187561035\n",
      "Epoch 18/400, Loss: 1.2263600826263428\n",
      "Epoch 19/400, Loss: 1.2246915102005005\n",
      "Epoch 20/400, Loss: 1.2259483337402344\n",
      "Epoch 21/400, Loss: 1.2263940572738647\n",
      "Epoch 22/400, Loss: 1.2279897928237915\n",
      "Epoch 23/400, Loss: 1.2257598638534546\n",
      "Epoch 24/400, Loss: 1.224772334098816\n",
      "Epoch 25/400, Loss: 1.2254890203475952\n",
      "Epoch 26/400, Loss: 1.2250566482543945\n",
      "Epoch 27/400, Loss: 1.2781251668930054\n",
      "Epoch 28/400, Loss: 1.2249420881271362\n",
      "Epoch 29/400, Loss: 1.225145697593689\n",
      "Epoch 30/400, Loss: 1.2287381887435913\n",
      "Epoch 31/400, Loss: 1.2286901473999023\n",
      "Epoch 32/400, Loss: 1.224534511566162\n",
      "Epoch 33/400, Loss: 1.2248804569244385\n",
      "Epoch 34/400, Loss: 1.22898268699646\n",
      "Epoch 35/400, Loss: 1.2251523733139038\n",
      "Epoch 36/400, Loss: 1.2251274585723877\n",
      "Epoch 37/400, Loss: 1.224534511566162\n",
      "Epoch 38/400, Loss: 1.2246931791305542\n",
      "Epoch 39/400, Loss: 1.2255066633224487\n",
      "Epoch 40/400, Loss: 1.2259694337844849\n",
      "Epoch 41/400, Loss: 1.2253402471542358\n",
      "Epoch 42/400, Loss: 1.225649356842041\n",
      "Epoch 43/400, Loss: 1.2246938943862915\n",
      "Epoch 44/400, Loss: 1.2249462604522705\n",
      "Epoch 45/400, Loss: 1.2820661067962646\n",
      "Epoch 46/400, Loss: 1.2250852584838867\n",
      "Epoch 47/400, Loss: 1.2259423732757568\n",
      "Epoch 48/400, Loss: 1.224941611289978\n",
      "Epoch 49/400, Loss: 1.224603295326233\n",
      "Epoch 50/400, Loss: 1.2247899770736694\n",
      "Epoch 51/400, Loss: 1.225647211074829\n",
      "Epoch 52/400, Loss: 1.2300270795822144\n",
      "Epoch 53/400, Loss: 1.2256423234939575\n",
      "Epoch 54/400, Loss: 1.2247928380966187\n",
      "Epoch 55/400, Loss: 1.2249412536621094\n",
      "Epoch 56/400, Loss: 1.2249422073364258\n",
      "Epoch 57/400, Loss: 1.2260260581970215\n",
      "Epoch 58/400, Loss: 1.2247858047485352\n",
      "Epoch 59/400, Loss: 1.224588394165039\n",
      "Epoch 60/400, Loss: 1.2286995649337769\n",
      "Epoch 61/400, Loss: 1.279163122177124\n",
      "Epoch 62/400, Loss: 1.225720763206482\n",
      "Epoch 63/400, Loss: 1.225821852684021\n",
      "Epoch 64/400, Loss: 1.225589394569397\n",
      "Epoch 65/400, Loss: 1.224592685699463\n",
      "Epoch 66/400, Loss: 1.2245899438858032\n",
      "Epoch 67/400, Loss: 1.2286114692687988\n",
      "Epoch 68/400, Loss: 1.2288957834243774\n",
      "Epoch 69/400, Loss: 1.2246898412704468\n",
      "Epoch 70/400, Loss: 1.2257094383239746\n",
      "Epoch 71/400, Loss: 1.2250750064849854\n",
      "Epoch 72/400, Loss: 1.2254866361618042\n",
      "Epoch 73/400, Loss: 1.2244994640350342\n",
      "Epoch 74/400, Loss: 1.2256348133087158\n",
      "Epoch 75/400, Loss: 1.22458815574646\n",
      "Epoch 76/400, Loss: 1.2254868745803833\n",
      "Epoch 77/400, Loss: 1.2244960069656372\n",
      "Epoch 78/400, Loss: 1.22869074344635\n",
      "Epoch 79/400, Loss: 1.225635051727295\n",
      "Epoch 80/400, Loss: 1.2246931791305542\n",
      "Epoch 81/400, Loss: 1.2260105609893799\n",
      "Epoch 82/400, Loss: 1.228605031967163\n",
      "Epoch 83/400, Loss: 1.225073218345642\n",
      "Epoch 84/400, Loss: 1.2254838943481445\n",
      "Epoch 85/400, Loss: 1.2256355285644531\n",
      "Epoch 86/400, Loss: 1.2244915962219238\n",
      "Epoch 87/400, Loss: 1.2256327867507935\n",
      "Epoch 88/400, Loss: 1.224783182144165\n",
      "Epoch 89/400, Loss: 1.2249301671981812\n",
      "Epoch 90/400, Loss: 1.2259280681610107\n",
      "Epoch 91/400, Loss: 1.2250738143920898\n",
      "Epoch 92/400, Loss: 1.2253092527389526\n",
      "Epoch 93/400, Loss: 1.2286031246185303\n",
      "Epoch 94/400, Loss: 1.226806402206421\n",
      "Epoch 95/400, Loss: 1.2247836589813232\n",
      "Epoch 96/400, Loss: 1.2251267433166504\n",
      "Epoch 97/400, Loss: 1.2244939804077148\n",
      "Epoch 98/400, Loss: 1.225071907043457\n",
      "Epoch 99/400, Loss: 1.2259238958358765\n",
      "Epoch 100/400, Loss: 1.2249306440353394\n",
      "Epoch 101/400, Loss: 1.2255784273147583\n",
      "Epoch 102/400, Loss: 1.2262177467346191\n",
      "Epoch 103/400, Loss: 1.2796911001205444\n",
      "Epoch 104/400, Loss: 1.2247847318649292\n",
      "Epoch 105/400, Loss: 1.2787972688674927\n",
      "Epoch 106/400, Loss: 1.2258249521255493\n",
      "Epoch 107/400, Loss: 1.225072979927063\n",
      "Epoch 108/400, Loss: 1.2262142896652222\n",
      "Epoch 109/400, Loss: 1.2286032438278198\n",
      "Epoch 110/400, Loss: 1.22512686252594\n",
      "Epoch 111/400, Loss: 1.228602647781372\n",
      "Epoch 112/400, Loss: 1.2259221076965332\n",
      "Epoch 113/400, Loss: 1.2251274585723877\n",
      "Epoch 114/400, Loss: 1.2246865034103394\n",
      "Epoch 115/400, Loss: 1.2254831790924072\n",
      "Epoch 116/400, Loss: 1.2255778312683105\n",
      "Epoch 117/400, Loss: 1.279365062713623\n",
      "Epoch 118/400, Loss: 1.2298871278762817\n",
      "Epoch 119/400, Loss: 1.2247872352600098\n",
      "Epoch 120/400, Loss: 1.2244919538497925\n",
      "Epoch 121/400, Loss: 1.226232886314392\n",
      "Epoch 122/400, Loss: 1.22478187084198\n",
      "Epoch 123/400, Loss: 1.2247819900512695\n",
      "Epoch 124/400, Loss: 1.2288936376571655\n",
      "Epoch 125/400, Loss: 1.2244915962219238\n",
      "Epoch 126/400, Loss: 1.2247823476791382\n",
      "Epoch 127/400, Loss: 1.2258405685424805\n",
      "Epoch 128/400, Loss: 1.2253122329711914\n",
      "Epoch 129/400, Loss: 1.2249401807785034\n",
      "Epoch 130/400, Loss: 1.2259248495101929\n",
      "Epoch 131/400, Loss: 1.2255784273147583\n",
      "Epoch 132/400, Loss: 1.2810399532318115\n",
      "Epoch 133/400, Loss: 1.2256371974945068\n",
      "Epoch 134/400, Loss: 1.2260106801986694\n",
      "Epoch 135/400, Loss: 1.225310206413269\n",
      "Epoch 136/400, Loss: 1.2254829406738281\n",
      "Epoch 137/400, Loss: 1.2245866060256958\n",
      "Epoch 138/400, Loss: 1.2247812747955322\n",
      "Epoch 139/400, Loss: 1.2250717878341675\n",
      "Epoch 140/400, Loss: 1.2258152961730957\n",
      "Epoch 141/400, Loss: 1.2251256704330444\n",
      "Epoch 142/400, Loss: 1.2254825830459595\n",
      "Epoch 143/400, Loss: 1.2267727851867676\n",
      "Epoch 144/400, Loss: 1.2286888360977173\n",
      "Epoch 145/400, Loss: 1.224585771560669\n",
      "Epoch 146/400, Loss: 1.2247809171676636\n",
      "Epoch 147/400, Loss: 1.225308895111084\n",
      "Epoch 148/400, Loss: 1.2286018133163452\n",
      "Epoch 149/400, Loss: 1.2253077030181885\n",
      "Epoch 150/400, Loss: 1.226775884628296\n",
      "Epoch 151/400, Loss: 1.2256327867507935\n",
      "Epoch 152/400, Loss: 1.2247812747955322\n",
      "Epoch 153/400, Loss: 1.226806640625\n",
      "Epoch 154/400, Loss: 1.2286888360977173\n",
      "Epoch 155/400, Loss: 1.2259221076965332\n",
      "Epoch 156/400, Loss: 1.2260096073150635\n",
      "Epoch 157/400, Loss: 1.2258142232894897\n",
      "Epoch 158/400, Loss: 1.225631833076477\n",
      "Epoch 159/400, Loss: 1.224780559539795\n",
      "Epoch 160/400, Loss: 1.2262152433395386\n",
      "Epoch 161/400, Loss: 1.2271356582641602\n",
      "Epoch 162/400, Loss: 1.2256321907043457\n",
      "Epoch 163/400, Loss: 1.225482702255249\n",
      "Epoch 164/400, Loss: 1.2286885976791382\n",
      "Epoch 165/400, Loss: 1.2258142232894897\n",
      "Epoch 166/400, Loss: 1.2247809171676636\n",
      "Epoch 167/400, Loss: 1.2247815132141113\n",
      "Epoch 168/400, Loss: 1.2247806787490845\n",
      "Epoch 169/400, Loss: 1.22548246383667\n",
      "Epoch 170/400, Loss: 1.2298305034637451\n",
      "Epoch 171/400, Loss: 1.2247811555862427\n",
      "Epoch 172/400, Loss: 1.2259230613708496\n",
      "Epoch 173/400, Loss: 1.2249301671981812\n",
      "Epoch 174/400, Loss: 1.2259224653244019\n",
      "Epoch 175/400, Loss: 1.2256317138671875\n",
      "Epoch 176/400, Loss: 1.224791407585144\n",
      "Epoch 177/400, Loss: 1.2293086051940918\n",
      "Epoch 178/400, Loss: 1.229088544845581\n",
      "Epoch 179/400, Loss: 1.2259221076965332\n",
      "Epoch 180/400, Loss: 1.2253074645996094\n",
      "Epoch 181/400, Loss: 1.226914882659912\n",
      "Epoch 182/400, Loss: 1.2251248359680176\n",
      "Epoch 183/400, Loss: 1.2260088920593262\n",
      "Epoch 184/400, Loss: 1.2258155345916748\n",
      "Epoch 185/400, Loss: 1.282598853111267\n",
      "Epoch 186/400, Loss: 1.2284530401229858\n",
      "Epoch 187/400, Loss: 1.2293076515197754\n",
      "Epoch 188/400, Loss: 1.22714102268219\n",
      "Epoch 189/400, Loss: 1.2249298095703125\n",
      "Epoch 190/400, Loss: 1.2285014390945435\n",
      "Epoch 191/400, Loss: 1.2245864868164062\n",
      "Epoch 192/400, Loss: 1.2251276969909668\n",
      "Epoch 193/400, Loss: 1.2258143424987793\n",
      "Epoch 194/400, Loss: 1.2246142625808716\n",
      "Epoch 195/400, Loss: 1.2247817516326904\n",
      "Epoch 196/400, Loss: 1.2254825830459595\n",
      "Epoch 197/400, Loss: 1.224585771560669\n",
      "Epoch 198/400, Loss: 1.22548246383667\n",
      "Epoch 199/400, Loss: 1.2256313562393188\n",
      "Epoch 200/400, Loss: 1.2251250743865967\n",
      "Epoch 201/400, Loss: 1.2289459705352783\n",
      "Epoch 202/400, Loss: 1.2251310348510742\n",
      "Epoch 203/400, Loss: 1.2247815132141113\n",
      "Epoch 204/400, Loss: 1.2244904041290283\n",
      "Epoch 205/400, Loss: 1.2262133359909058\n",
      "Epoch 206/400, Loss: 1.2250714302062988\n",
      "Epoch 207/400, Loss: 1.226914882659912\n",
      "Epoch 208/400, Loss: 1.2251248359680176\n",
      "Epoch 209/400, Loss: 1.2251250743865967\n",
      "Epoch 210/400, Loss: 1.2244900465011597\n",
      "Epoch 211/400, Loss: 1.224780559539795\n",
      "Epoch 212/400, Loss: 1.2246851921081543\n",
      "Epoch 213/400, Loss: 1.2254825830459595\n",
      "Epoch 214/400, Loss: 1.2256313562393188\n",
      "Epoch 215/400, Loss: 1.2260088920593262\n",
      "Epoch 216/400, Loss: 1.224780559539795\n",
      "Epoch 217/400, Loss: 1.2247806787490845\n",
      "Epoch 218/400, Loss: 1.225577712059021\n",
      "Epoch 219/400, Loss: 1.22548246383667\n",
      "Epoch 220/400, Loss: 1.2284525632858276\n",
      "Epoch 221/400, Loss: 1.2286016941070557\n",
      "Epoch 222/400, Loss: 1.2286015748977661\n",
      "Epoch 223/400, Loss: 1.224780797958374\n",
      "Epoch 224/400, Loss: 1.224780559539795\n",
      "Epoch 225/400, Loss: 1.2256312370300293\n",
      "Epoch 226/400, Loss: 1.2253074645996094\n",
      "Epoch 227/400, Loss: 1.2251253128051758\n",
      "Epoch 228/400, Loss: 1.2254825830459595\n",
      "Epoch 229/400, Loss: 1.2246851921081543\n",
      "Epoch 230/400, Loss: 1.2244900465011597\n",
      "Epoch 231/400, Loss: 1.2249298095703125\n",
      "Epoch 232/400, Loss: 1.2244902849197388\n",
      "Epoch 233/400, Loss: 1.2271342277526855\n",
      "Epoch 234/400, Loss: 1.2791085243225098\n",
      "Epoch 235/400, Loss: 1.2244926691055298\n",
      "Epoch 236/400, Loss: 1.2255780696868896\n",
      "Epoch 237/400, Loss: 1.226806402206421\n",
      "Epoch 238/400, Loss: 1.2260088920593262\n",
      "Epoch 239/400, Loss: 1.2249304056167603\n",
      "Epoch 240/400, Loss: 1.225631594657898\n",
      "Epoch 241/400, Loss: 1.2249298095703125\n",
      "Epoch 242/400, Loss: 1.2256314754486084\n",
      "Epoch 243/400, Loss: 1.2259225845336914\n",
      "Epoch 244/400, Loss: 1.2286909818649292\n",
      "Epoch 245/400, Loss: 1.2247806787490845\n",
      "Epoch 246/400, Loss: 1.2247806787490845\n",
      "Epoch 247/400, Loss: 1.2251253128051758\n",
      "Epoch 248/400, Loss: 1.2251249551773071\n",
      "Epoch 249/400, Loss: 1.2247806787490845\n",
      "Epoch 250/400, Loss: 1.2247809171676636\n",
      "Epoch 251/400, Loss: 1.2267727851867676\n",
      "Epoch 252/400, Loss: 1.2255778312683105\n",
      "Epoch 253/400, Loss: 1.2251255512237549\n",
      "Epoch 254/400, Loss: 1.2293076515197754\n",
      "Epoch 255/400, Loss: 1.225813627243042\n",
      "Epoch 256/400, Loss: 1.2286885976791382\n",
      "Epoch 257/400, Loss: 1.2262132167816162\n",
      "Epoch 258/400, Loss: 1.2256312370300293\n",
      "Epoch 259/400, Loss: 1.2244900465011597\n",
      "Epoch 260/400, Loss: 1.2251250743865967\n",
      "Epoch 261/400, Loss: 1.232979416847229\n",
      "Epoch 262/400, Loss: 1.2247813940048218\n",
      "Epoch 263/400, Loss: 1.280043125152588\n",
      "Epoch 264/400, Loss: 1.2246854305267334\n",
      "Epoch 265/400, Loss: 1.225577712059021\n",
      "Epoch 266/400, Loss: 1.224929928779602\n",
      "Epoch 267/400, Loss: 1.2247806787490845\n",
      "Epoch 268/400, Loss: 1.2253998517990112\n",
      "Epoch 269/400, Loss: 1.2793915271759033\n",
      "Epoch 270/400, Loss: 1.2256325483322144\n",
      "Epoch 271/400, Loss: 1.225307583808899\n",
      "Epoch 272/400, Loss: 1.225308895111084\n",
      "Epoch 273/400, Loss: 1.224780797958374\n",
      "Epoch 274/400, Loss: 1.2247810363769531\n",
      "Epoch 275/400, Loss: 1.2249301671981812\n",
      "Epoch 276/400, Loss: 1.224929928779602\n",
      "Epoch 277/400, Loss: 1.2244901657104492\n",
      "Epoch 278/400, Loss: 1.2246856689453125\n",
      "Epoch 279/400, Loss: 1.2247806787490845\n",
      "Epoch 280/400, Loss: 1.2247806787490845\n",
      "Epoch 281/400, Loss: 1.2247806787490845\n",
      "Epoch 282/400, Loss: 1.224780559539795\n",
      "Epoch 283/400, Loss: 1.2286016941070557\n",
      "Epoch 284/400, Loss: 1.228979229927063\n",
      "Epoch 285/400, Loss: 1.227134346961975\n",
      "Epoch 286/400, Loss: 1.224780559539795\n",
      "Epoch 287/400, Loss: 1.2269147634506226\n",
      "Epoch 288/400, Loss: 1.2245855331420898\n",
      "Epoch 289/400, Loss: 1.2244898080825806\n",
      "Epoch 290/400, Loss: 1.2254823446273804\n",
      "Epoch 291/400, Loss: 1.224780559539795\n",
      "Epoch 292/400, Loss: 1.228506326675415\n",
      "Epoch 293/400, Loss: 1.2247806787490845\n",
      "Epoch 294/400, Loss: 1.2244899272918701\n",
      "Epoch 295/400, Loss: 1.225577712059021\n",
      "Epoch 296/400, Loss: 1.232909083366394\n",
      "Epoch 297/400, Loss: 1.2251248359680176\n",
      "Epoch 298/400, Loss: 1.224780559539795\n",
      "Epoch 299/400, Loss: 1.2251249551773071\n",
      "Epoch 300/400, Loss: 1.224585771560669\n",
      "Epoch 301/400, Loss: 1.2247806787490845\n",
      "Epoch 302/400, Loss: 1.224929690361023\n",
      "Epoch 303/400, Loss: 1.2244899272918701\n",
      "Epoch 304/400, Loss: 1.2256312370300293\n",
      "Epoch 305/400, Loss: 1.224780559539795\n",
      "Epoch 306/400, Loss: 1.2286885976791382\n",
      "Epoch 307/400, Loss: 1.2260093688964844\n",
      "Epoch 308/400, Loss: 1.2286018133163452\n",
      "Epoch 309/400, Loss: 1.226914882659912\n",
      "Epoch 310/400, Loss: 1.224929690361023\n",
      "Epoch 311/400, Loss: 1.2289460897445679\n",
      "Epoch 312/400, Loss: 1.224780559539795\n",
      "Epoch 313/400, Loss: 1.2244899272918701\n",
      "Epoch 314/400, Loss: 1.2255785465240479\n",
      "Epoch 315/400, Loss: 1.2251255512237549\n",
      "Epoch 316/400, Loss: 1.2286887168884277\n",
      "Epoch 317/400, Loss: 1.225631594657898\n",
      "Epoch 318/400, Loss: 1.2256314754486084\n",
      "Epoch 319/400, Loss: 1.2247806787490845\n",
      "Epoch 320/400, Loss: 1.2256313562393188\n",
      "Epoch 321/400, Loss: 1.2260088920593262\n",
      "Epoch 322/400, Loss: 1.2255793809890747\n",
      "Epoch 323/400, Loss: 1.226212978363037\n",
      "Epoch 324/400, Loss: 1.2253071069717407\n",
      "Epoch 325/400, Loss: 1.2253071069717407\n",
      "Epoch 326/400, Loss: 1.2293075323104858\n",
      "Epoch 327/400, Loss: 1.2254821062088013\n",
      "Epoch 328/400, Loss: 1.224929690361023\n",
      "Epoch 329/400, Loss: 1.2254823446273804\n",
      "Epoch 330/400, Loss: 1.2247806787490845\n",
      "Epoch 331/400, Loss: 1.224780559539795\n",
      "Epoch 332/400, Loss: 1.2251248359680176\n",
      "Epoch 333/400, Loss: 1.2246851921081543\n",
      "Epoch 334/400, Loss: 1.2260087728500366\n",
      "Epoch 335/400, Loss: 1.2254822254180908\n",
      "Epoch 336/400, Loss: 1.2256312370300293\n",
      "Epoch 337/400, Loss: 1.2253073453903198\n",
      "Epoch 338/400, Loss: 1.2284525632858276\n",
      "Epoch 339/400, Loss: 1.2259219884872437\n",
      "Epoch 340/400, Loss: 1.2250713109970093\n",
      "Epoch 341/400, Loss: 1.2293075323104858\n",
      "Epoch 342/400, Loss: 1.224780559539795\n",
      "Epoch 343/400, Loss: 1.224929690361023\n",
      "Epoch 344/400, Loss: 1.226212978363037\n",
      "Epoch 345/400, Loss: 1.2269147634506226\n",
      "Epoch 346/400, Loss: 1.2244900465011597\n",
      "Epoch 347/400, Loss: 1.2256312370300293\n",
      "Epoch 348/400, Loss: 1.224780797958374\n",
      "Epoch 349/400, Loss: 1.278026819229126\n",
      "Epoch 350/400, Loss: 1.2244908809661865\n",
      "Epoch 351/400, Loss: 1.2259221076965332\n",
      "Epoch 352/400, Loss: 1.2253073453903198\n",
      "Epoch 353/400, Loss: 1.2290881872177124\n",
      "Epoch 354/400, Loss: 1.2260088920593262\n",
      "Epoch 355/400, Loss: 1.225921869277954\n",
      "Epoch 356/400, Loss: 1.226914644241333\n",
      "Epoch 357/400, Loss: 1.2245851755142212\n",
      "Epoch 358/400, Loss: 1.225813627243042\n",
      "Epoch 359/400, Loss: 1.2256349325180054\n",
      "Epoch 360/400, Loss: 1.2254822254180908\n",
      "Epoch 361/400, Loss: 1.2255775928497314\n",
      "Epoch 362/400, Loss: 1.2246851921081543\n",
      "Epoch 363/400, Loss: 1.229308009147644\n",
      "Epoch 364/400, Loss: 1.2251248359680176\n",
      "Epoch 365/400, Loss: 1.224780559539795\n",
      "Epoch 366/400, Loss: 1.2253072261810303\n",
      "Epoch 367/400, Loss: 1.2259219884872437\n",
      "Epoch 368/400, Loss: 1.2254821062088013\n",
      "Epoch 369/400, Loss: 1.2260087728500366\n",
      "Epoch 370/400, Loss: 1.2259219884872437\n",
      "Epoch 371/400, Loss: 1.2289460897445679\n",
      "Epoch 372/400, Loss: 1.2247806787490845\n",
      "Epoch 373/400, Loss: 1.2255778312683105\n",
      "Epoch 374/400, Loss: 1.2259221076965332\n",
      "Epoch 375/400, Loss: 1.2286887168884277\n",
      "Epoch 376/400, Loss: 1.224780559539795\n",
      "Epoch 377/400, Loss: 1.225813627243042\n",
      "Epoch 378/400, Loss: 1.2284523248672485\n",
      "Epoch 379/400, Loss: 1.2244901657104492\n",
      "Epoch 380/400, Loss: 1.224780559539795\n",
      "Epoch 381/400, Loss: 1.224780559539795\n",
      "Epoch 382/400, Loss: 1.2253072261810303\n",
      "Epoch 383/400, Loss: 1.2266278266906738\n",
      "Epoch 384/400, Loss: 1.2260087728500366\n",
      "Epoch 385/400, Loss: 1.2255775928497314\n",
      "Epoch 386/400, Loss: 1.2302395105361938\n",
      "Epoch 387/400, Loss: 1.225921869277954\n",
      "Epoch 388/400, Loss: 1.2250711917877197\n",
      "Epoch 389/400, Loss: 1.230008840560913\n",
      "Epoch 390/400, Loss: 1.224780559539795\n",
      "Epoch 391/400, Loss: 1.2794183492660522\n",
      "Epoch 392/400, Loss: 1.2247806787490845\n",
      "Epoch 393/400, Loss: 1.2246850728988647\n",
      "Epoch 394/400, Loss: 1.2256313562393188\n",
      "Epoch 395/400, Loss: 1.225921869277954\n",
      "Epoch 396/400, Loss: 1.2251248359680176\n",
      "Epoch 397/400, Loss: 1.2246851921081543\n",
      "Epoch 398/400, Loss: 1.2253072261810303\n",
      "Epoch 399/400, Loss: 1.2251248359680176\n",
      "Epoch 400/400, Loss: 1.2244899272918701\n",
      "Global Accuracy: 1.00\n",
      "Accuracy of class a: 1.00\n",
      "Accuracy of class b: 1.00\n",
      "Accuracy of class c: 1.00\n",
      "Accuracy of class d: 1.00\n"
     ]
    }
   ],
   "source": [
    "evaluate_accuracy()\n",
    "# Training loop\n",
    "num_epochs = 400\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "evaluate_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "torch.save(model.state_dict(), 'handmodel.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
